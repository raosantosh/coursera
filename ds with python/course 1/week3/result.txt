This assignment was graded with Grader version 2016.12.30a. If you don't understand the grader message, or believe the message to be inaccurate, please see the discussion forums for details on the grader limitations. If you want your assignment results to be verified, you must download the notebook and submit it manually, Coursera is working on this feature. See the course resources Jupyter Notebook FAQ page for more details.

 Detected Jupyter notebook submission.
 
----------
 Function answer_one results were not the same as the solution results. Iterating through the dataframes we found that there were differences in the following columns: dict_keys(['Energy Supply', 'Energy Supply per Capita']). 
----------
 Function answer_one was answered incorrectly, 0.2 points were not awarded. The shape of the DataFrame was correct, and the column names were correct, but equality testing failed. 
----------
 Function answer_two was answered incorrectly, 0.0666 points were not awarded. 
----------
 Function answer_three results were not the same as the solution results. Iterating through the series we found that there were 1 differences. 
----------
 Function answer_three was answered incorrectly, 0.0666 points were not awarded 
----------
 Function answer_four was answered incorrectly, you returned a variable of type <class 'int'> and we expected a type of <class 'numpy.float64'>. 0.0666 points we not awarded. 
----------
 Function answer_five was answered incorrectly, 0.0666 points were not awarded. 
----------
 Function answer_six was answered incorrectly, you returned a variable of type <class 'pandas.core.series.Series'> and we expected a type of <class 'tuple'>. 0.0666 points we not awarded. 
----------
 Function answer_seven was answered incorrectly, you returned a variable of type <class 'pandas.core.series.Series'> and we expected a type of <class 'tuple'>. 0.0666 points we not awarded. 
----------
 Function answer_eight was answered incorrectly, 0.0666 points were not awarded. 
----------
 Function answer_nine was answered incorrectly, 0.0666 points were not awarded. 
----------
 Function answer_ten results were not the same as the solution results. Iterating through the series we found that there were 1 differences. 
----------
 Function answer_ten was answered incorrectly, 0.0666 points were not awarded 
----------
 Function answer_eleven results were not the same as the solution results. Iterating through the dataframes we found that there were differences in the following columns: dict_keys(['sum', 'mean']). 
----------
 Function answer_eleven was answered incorrectly, 0.0666 points were not awarded. The shape of the DataFrame was correct, and the column names were correct, but equality testing failed. 
----------
 Warning (function answer_twelve), your Series is of type float64, but ours is of type int64. Attempting to convert for grading. 
----------
 Function answer_twelve results were not the same as the solution results. Iterating through the series we found that there were 2 differences. 
----------
 Function answer_twelve was answered incorrectly, 0.0666 points were not awarded 
----------
 Function answer_thirteen results were not the same as the solution results. Iterating through the series we found that there were 15 differences. 
----------
 Function answer_thirteen was answered incorrectly, 0.0674 points were not awarded 
----------
